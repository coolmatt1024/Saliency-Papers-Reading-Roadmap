# 1 作者的出发点


**深度学习出发点就是自动学习特征和将简单特征自动组合成高级特征。**

CNN学习特征是卷积+激活函数得到feature map，然后通过全连接层来对特征进行分类

**那么CNN得到的特征抽象水平是怎样的呢？？？【这个好像很少有人关注过】**

作者理论分析了一下，认为CNN是一个泛化的线性模型（GLM），得到的特征抽象层次比较低（当样本线性可分时CNN抽象能力比较好）【这里不太理解，对传统机器学习了的特征理解不太深】

所以，作者就提出了将GLM替换成一个more potent nonlinear function approximator，也就是文中提到的micro network


# 2 Convolutional Neural Network

> 当潜在概念的实例是线性可分时，线性卷积用于抽取是足够的，然而，好的抽象表示一般是高度非线性函数。在传统cnn，这一点可以用超完备过滤的方式。即：增加过滤器，但增加过滤器会增加下层的计算负担。

这一段的理解可能要对传统机器学习有很深的理解吧（本来以为深度学习没啥理论知识，但现在看来，起码对于网络的改进，都要有传统机器学习的基础啊）。好像就是，如果你提取的都是些边啊，那么为了组合得到更复杂的特征，那么你的这些基本特征的量要足够多。。。。。

这里还提到了Maxout的方法，感兴趣可以看论文或者博客
[Network in Network-读后笔记][1]

# 3 mlpconv layer

在不知道输入数据的先验分布时，用通用的函数近似器来提取特征是比较好的方法，如径向基函数和多层感知器（这里传统的提取特征的方法就只有这些吗？？？）

**之所以选用MLP：**
 - MLP与cnn相兼容 
 - MLP可自行深度化

## 3.1 结构

![此处输入图片的描述][2]

CNN：卷积得到feature map
mlpconv：卷积+mlp(2层)

其实，这个图表示的不是很清楚

![此处输入图片的描述][3]

**简单来说，mlpconv也是局部连接的形式，对于这一小块接收野，用一个多层感知器来得到对应的特征**

## 3.2 与普通CNN的关系

作者又提到，这种多层感知器可以用普通的卷积层来实现（只是卷积核变成了1x1，而且特征维度也不需要那么多）

![此处输入图片的描述][4]

若前一层输入2x（4x4）
第一层卷积核2x2，步长为1，通道数为4
卷积得到4x(3x3)（图上只是画了一个patch的情况）
第二层卷积核1x1，步长为1，通道数为3
卷积得到3x(3x3)（图上还是只画了一个patch）
第三层卷积核为1x1，步长为1，通道数为2
卷积得到2x(3x3)

这样，总的权重为2x2x2x4+1x1x4x3+1x1x3x2，其实如果通道数减少了，那么参数相应地也会减少很多

## 3.3 1x1卷积的作用

 - 实现跨通道的交互和信息整合 
 - 进行卷积核通道数的降维和升维（GoogleNet，ResNet）

# 4 全局平均池化

![此处输入图片的描述][5]

传统的CNN将卷积层当做特征提取器，分类用传统的方法来实现（softmax），这样全连接层很容易出现过拟合。可以使用Dropout来减轻过拟合，这里作者使用了另外一种思路。

作者**强化了特征图和类的关系**，为每一类生成了一个feature map，而且**减轻了过拟合现象**




  [1]: http://www.jianshu.com/p/96791a306ea5anshu.com/p/96791a306ea5
  [2]: http://img1.ph.126.net/U8J-KGzkktwuNFY4XREVBw==/6632424563049920905.jpg
  [3]: http://images2015.cnblogs.com/blog/961754/201706/961754-20170610113107778-221549838.png
  [4]: http://images2015.cnblogs.com/blog/961754/201706/961754-20170610113109137-2111454790.png
  [5]: http://images2015.cnblogs.com/blog/961754/201706/961754-20170610113109856-1451092228.png
